{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0e0f9f",
   "metadata": {},
   "source": [
    "# Doomed to failure: A story about target encoding and Tree-Based Algorithms\n",
    "---\n",
    "\n",
    "Have you ever worked with target encoders? Did you ever use any tree-based model? If you have worked with either there will likely be a time in which you are tempted to use both in the same pipeline for classifying data points with categorical features. DON'T! At least until you have read this article detailing one dangerous caveat that this combination has.\n",
    "\n",
    "In this article, I will guide you through the caveats that appear when a pipeline combines certain members of the family of target encoders with tree-based models and extremely low or high entropy features. I'll clue you in: data leakeages and overfitting issues will break your pipeline's performance. \n",
    "\n",
    "The good news is that these problems that we will see are easy to solve - just use CatBoost and let it handle the encoding methodology for you. As I will be arguing the encoder that it uses by default for categorical features magnificently handles categorical fetures with both extremely low and high entropies and is still in the realm of target encoding.\n",
    "\n",
    "**Important: to simplify the discussion we will frame the discussion under a binary classification task.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b59401",
   "metadata": {},
   "source": [
    "#### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dcffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fde2a",
   "metadata": {},
   "source": [
    "## Background Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0557f3",
   "metadata": {},
   "source": [
    "### Encoders\n",
    "\n",
    "Encoding methodologies aim to transform non-numerical categorical feature values into numerical values. There are multiple approaches and each comes with its own advantages and caveats. For instance, to mention a few of them:\n",
    "\n",
    "- **Label Encoding**: it simply assigns a unique integer to each category in the data.\n",
    "    *Pros:*\n",
    "    1. Simple\n",
    "\n",
    "    *Cons:*\n",
    "    1. May introduce ordinal relationships\n",
    "    2. Features with unknown amounts of categories which have high amounts of low-frecquency categories may be clustered into a marginal category, loosing predictive power based on them.\n",
    "\n",
    "- **One-Hot Encoding**: for each category seen for a feature, it creates a new dummy feature which simply indicates with 0 or 1 when the category appears.\n",
    "    *Pros:*\n",
    "    1. Simple\n",
    "    2. Does not assume order between categories\n",
    "\n",
    "    *Cons:*\n",
    "    1. Dimensionality may increase considerably making the use of a dimensionality reduction technique necessary and potentially exploding your system's memmory (tip: if you still want to use it check sparse matrices).\n",
    "    2. Computationally expensive.\n",
    "    3. Features with unknown amounts of categories which have high amounts of low-frecquency categories may be clustered into a marginal category, loosing predictive power based on them.\n",
    "\n",
    "- **Binary Encoding, Hashing Encodings and more. Let's not get carried out into an endless enumeration.**\n",
    "\n",
    "Importantly, encoding methodologies and classifier interact in different ways and consequently the resulting pipelines may inherit or develop different properties and caveats. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eadb1f5",
   "metadata": {},
   "source": [
    "### Target Encoders\n",
    "\n",
    "In its most basic form, a target encoder substitutes each category $c$ of a feature with the training set statistic \n",
    "$$\\frac{N_{positive\\_samples\\_with\\_category\\_c\\_for\\_the\\_feature}}{N_{all\\_samples\\_with\\_category\\_c\\_for\\_the\\_feature}}$$\n",
    "\n",
    "Let us see this basic encoder in action with a basic custom implementation. Firstly, we create a trivial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c98a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature_1  target\n",
       "0         A       0\n",
       "1         B       1\n",
       "2         A       1\n",
       "3         C       1\n",
       "4         B       0\n",
       "5         A       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_1 = pd.DataFrame(\n",
    "    {\n",
    "        \"feature_1\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\"],\n",
    "        \"target\": [0, 1, 1, 1, 0, 0],\n",
    "    }\n",
    ")\n",
    "dataset_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7859734c",
   "metadata": {},
   "source": [
    "Then, we implement the encoding of the categories according to the most basic target encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f345934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>target</th>\n",
       "      <th>feature_1_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature_1  target  feature_1_encoded\n",
       "0         A       0           0.333333\n",
       "1         B       1           0.500000\n",
       "2         A       1           0.333333\n",
       "3         C       1           1.000000\n",
       "4         B       0           0.500000\n",
       "5         A       0           0.333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = dataset_1.feature_1.unique()\n",
    "stats_map = {\n",
    "    category: dataset_1[dataset_1.feature_1 == category].target.mean() for category in categories\n",
    "}\n",
    "dataset_1[\"feature_1_encoded\"] = dataset_1.feature_1.map(stats_map)\n",
    "dataset_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf10a1",
   "metadata": {},
   "source": [
    "The resulting encoded feature is the only one used by ML algorithms. The idea is that we can actually encode features by their tendency to be associated with the positive class. This way an algorithm learns on top of the tendencies seen for all features, \"weights\" them and is able to generalize on nobel data points\n",
    "\n",
    "Nevertheless, note that the encoded value for C has quite a strong meaning taking into account that we have only one instance of it (I know: the other categories are nearly as scarce from the statistical perspective). In other words, C could actually be a lucky sample and in reality C could actually be more associated with zero labels. Good catch! Actually, this first encoder comes with its own caveat: it is easy to overfit based on scarce categories. \n",
    "\n",
    "Therefore, many versions arised to distill this knowledge about category frecquency into the encodings. The most common approach is to simply perform a smoothing between the prior prevalence computed over the training set and the target statistic. For instance, the target encoding from the category-encoders library uses the smoothing:\n",
    "$$enc(category) = p * stat + (1 - p) * prevalence$$\n",
    "where:\n",
    "$$p = \\frac{1}{1 + \\exp\\big(\\frac{(-n\\_{value} - k)}{f}\\big)}$$\n",
    "and $k$ and $f$ are respectively called the minimum samples per leaf and smoothing parameters. To get further information checkout their [documentation](https://contrib.scikit-learn.org/category_encoders/targetencoder.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "towards-catboost-py3.11 (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
